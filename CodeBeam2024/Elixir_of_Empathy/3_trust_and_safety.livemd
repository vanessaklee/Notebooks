<!-- livebook:{"file_entries":[{"file":{"file_system_id":"local","file_system_type":"local","path":"/Users/vlee/Desktop/arizaona.png"},"name":"arizona.png","type":"file"},{"file":{"file_system_id":"local","file_system_type":"local","path":"/Users/vlee/Projects/companion/priv/static/images/govt_ids/feet.jpeg"},"name":"feet.jpeg","type":"file"},{"file":{"file_system_id":"local","file_system_type":"local","path":"/Users/vlee/Projects/companion/priv/static/images/govt_ids/maryland.jpeg"},"name":"maryland.jpeg","type":"file"},{"file":{"file_system_id":"local","file_system_type":"local","path":"/Users/vlee/Projects/companion/priv/static/images/govt_ids/rewards_card.png"},"name":"rewards_card.png","type":"file"},{"file":{"file_system_id":"local","file_system_type":"local","path":"/Users/vlee/Projects/companion/priv/static/images/govt_ids/scary.jpeg"},"name":"scary.jpeg","type":"file"},{"file":{"file_system_id":"local","file_system_type":"local","path":"/Users/vlee/Projects/companion/priv/static/images/govt_ids/school_id.png"},"name":"school_id.png","type":"file"},{"file":{"file_system_id":"local","file_system_type":"local","path":"/Users/vlee/Projects/companion/priv/static/images/govt_ids/texas.png"},"name":"texas.png","type":"file"},{"name":"us_id.png","type":"attachment"}]} -->

# 3. Trust & Safety

```elixir
Mix.install(
  [
    {:kino_bumblebee, "~> 0.4.0"},
    {:exla, ">= 0.0.0"},
    {:stb_image, "~> 0.6.2"},
    {:kino_vega_lite, "~> 0.1.10"}
  ],
  config: [nx: [default_backend: EXLA.Backend]]
)
```

## Smart Cell: Neural Network tasks

Use a "Neural Network task" Smart Cell to verify uploaded images. We'll use the "Image-to-text" task using the BLIP (base) - image captioning model from Hugging Face.

<!-- livebook:{"attrs":{"compiler":"exla","max_new_tokens":100,"min_new_tokens":1,"task_id":"image_to_text","variant_id":"blip_captioning_base"},"chunks":[[0,660],[662,617]],"kind":"Elixir.KinoBumblebee.TaskCell","livebook_object":"smart_cell"} -->

```elixir
{:ok, model_info} = Bumblebee.load_model({:hf, "Salesforce/blip-image-captioning-base"})

{:ok, featurizer} =
  Bumblebee.load_featurizer({:hf, "Salesforce/blip-image-captioning-base"})

{:ok, tokenizer} =
  Bumblebee.load_tokenizer({:hf, "Salesforce/blip-image-captioning-base"})

{:ok, generation_config} =
  Bumblebee.load_generation_config({:hf, "Salesforce/blip-image-captioning-base"})

generation_config =
  Bumblebee.configure(generation_config, min_new_tokens: 1, max_new_tokens: 100)

serving =
  Bumblebee.Vision.image_to_text(model_info, featurizer, tokenizer, generation_config,
    compile: [batch_size: 1],
    defn_options: [compiler: EXLA]
  )

image_input = Kino.Input.image("Image", size: {384, 384})
form = Kino.Control.form([image: image_input], submit: "Run")
frame = Kino.Frame.new()

Kino.listen(form, fn %{data: %{image: image}} ->
  if image do
    Kino.Frame.render(frame, Kino.Text.new("Running..."))

    image =
      image.file_ref
      |> Kino.Input.file_path()
      |> File.read!()
      |> Nx.from_binary(:u8)
      |> Nx.reshape({image.height, image.width, 3})

    %{results: [%{text: text}]} = Nx.Serving.run(serving, image)
    Kino.Frame.render(frame, Kino.Text.new(text))
  end
end)

Kino.Layout.grid([form, frame], boxed: true, gap: 16)
```

But using the Smart Cell still requires viewing the image. In the next section is an alternative method using Bumblebee that keeps the images out of sight.

## Bumblebee Vision Image-to-Text

### Out of Sight

Axon is an Elixir Library for Nx-powered Neural Networks.

Bumblebee is an Elixir library that provides pre-trained Neural Network models on top of Axon. It includes integration with Hugging Face ðŸ¤— Models, allowing anyone to download and perform Machine Learning tasks with few lines of code.

```elixir
repo = {:hf, "Salesforce/blip-image-captioning-base"}
{:ok, model_info} = Bumblebee.load_model(repo)
{:ok, featurizer} = Bumblebee.load_featurizer(repo)
{:ok, tokenizer} = Bumblebee.load_tokenizer(repo)
{:ok, generation_config} = Bumblebee.load_generation_config(repo)
generation_config = Bumblebee.configure(generation_config, max_new_tokens: 100)

serving =
  Bumblebee.Vision.image_to_text(model_info, featurizer, tokenizer, generation_config,
    compile: [batch_size: 1],
    defn_options: [compiler: EXLA]
  )
```

```elixir
data =
  [
    "maryland.jpeg",
    "feet.jpeg",
    "texas.png",
    "scary.jpeg",
    "arizona.png",
    "rewards_card.png",
    "school_id.png"
  ]
  |> Enum.map(fn img ->
    image = Kino.FS.file_path(img) |> StbImage.read_file!()

    case Nx.Serving.run(serving, image) do
      %{results: [%{text: text}]} ->
        %{caption: text, file: img}

      _ ->
        %{caption: "Unable to create caption", file: img}
    end
  end)

Kino.DataTable.new(
  data,
  keys: [:file, :caption],
  name: "Gov't ID Uploads with Captions"
)
```

```elixir
dataset =
  Enum.map(data, fn %{file: _f, caption: c} = m ->
    d = String.downcase(c)

    pass =
      if (String.contains?(d, "driver") and String.contains?(d, "id")) or
           (String.contains?(d, "passpord") and String.contains?(d, "id")) or
           (String.contains?(d, "government") and String.contains?(d, "id")) or
           (String.contains?(d, "state") and String.contains?(d, "id")) or
           String.contains?(d, ["passport", "driver", "identification"]) do
        true
      else
        false
      end

    Map.put_new(m, :verified_by_ai, pass)
  end)

vega_dataset =
  Enum.reduce(dataset, %{good: 0, bad: 0}, fn d, acc ->
    if d.verified_by_ai do
      Map.put(acc, :good, acc.good + 1)
    else
      Map.put(acc, :bad, acc.bad + 1)
    end
  end)

vega_dataset = [
  %{category: "Good", value: vega_dataset.good},
  %{category: "Bad", value: vega_dataset.bad}
]
```

```elixir
alias VegaLite, as: Vl

Vl.new()
|> Vl.data_from_values(vega_dataset)
|> Vl.mark(:arc)
|> Vl.encode_field(:theta, "value", type: :quantitative)
|> Vl.encode_field(:color, "category", type: :nominal)
|> Vl.config(view: [stroke: nil])
```

<!-- livebook:{"attrs":{"chart_title":"Verified Government ID Uploads","height":200,"layers":[{"active":true,"chart_type":"bar","color_field":"verified_by_ai","color_field_aggregate":null,"color_field_bin":null,"color_field_scale_scheme":null,"color_field_type":"nominal","data_variable":"dataset","geodata_color":"blue","latitude_field":null,"longitude_field":null,"x_field":"verified_by_ai","x_field_aggregate":null,"x_field_bin":null,"x_field_scale_type":null,"x_field_type":"nominal","y_field":"__count__","y_field_aggregate":null,"y_field_bin":null,"y_field_scale_type":null,"y_field_type":null}],"vl_alias":"Elixir.Vl","width":300},"chunks":null,"kind":"Elixir.KinoVegaLite.ChartCell","livebook_object":"smart_cell"} -->

```elixir
Vl.new(width: 300, height: 200, title: "Verified Government ID Uploads")
|> Vl.data_from_values(dataset, only: ["verified_by_ai"])
|> Vl.mark(:bar)
|> Vl.encode_field(:x, "verified_by_ai", type: :nominal)
|> Vl.encode(:y, aggregate: :count)
|> Vl.encode_field(:color, "verified_by_ai", type: :nominal)
```

<!-- livebook:{"offset":7071,"stamp":{"token":"XCP.5ApZwhoaLOpH6aZTFz5p6IXSnWRj30G4xVZwWL4fMEY-Y64JMvZ5yXRa6HdTVJc-bTDqCjIzc4rMHFTQ0tcgetQhf7Y_u-YAMKtdLQ","version":2}} -->
